{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using nn.Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "X = torch.arange(start=1, end=100, step=0.3).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "# Spliting data to train and test\n",
    "train_split = int(0.7 * len(X))\n",
    "x_train, y_train = X[:train_split], y[:train_split]\n",
    "x_test, y_test = X[train_split:], y[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearLayerRegressionModel(\n",
       "   (linearLayer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linearLayer.weight', tensor([[0.7645]])),\n",
       "              ('linearLayer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a linear model by subclassing nn.Module\n",
    "class LinearLayerRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        self.linearLayer = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linearLayer(X)\n",
    "\n",
    "# Set the manual seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Creating the instance of the model\n",
    "linearModel = LinearLayerRegressionModel()\n",
    "\n",
    "linearModel,linearModel.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"GPU\" for model if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device to cuda if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model's current device\n",
    "next(linearModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to the device\n",
    "linearModel.to(device)\n",
    "\n",
    "next(linearModel.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "1. Loss function\n",
    "2. Optimizer\n",
    "3. Training loop\n",
    "4. Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "lossFunction = nn.L1Loss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(linearModel.parameters(), lr=0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train device: cpu\n",
      "x_test device: cpu\n",
      "y_train device: cpu\n",
      "y_test device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check the device of each data\n",
    "print(f\"x_train device: {x_train.device}\")\n",
    "print(f\"x_test device: {x_test.device}\")\n",
    "print(f\"y_train device: {y_train.device}\")\n",
    "print(f\"y_test device: {y_test.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data to the device --> Because we are using GPU for train the model\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.8114452362060547, Test Loss: 6.0031023025512695\n",
      "Epoch: 10, Loss: 2.798941135406494, Test Loss: 5.973101615905762\n",
      "Epoch: 20, Loss: 2.786435842514038, Test Loss: 5.943100452423096\n",
      "Epoch: 30, Loss: 2.7739312648773193, Test Loss: 5.913099765777588\n",
      "Epoch: 40, Loss: 2.7614264488220215, Test Loss: 5.883098602294922\n",
      "Epoch: 50, Loss: 2.7489216327667236, Test Loss: 5.853097915649414\n",
      "Epoch: 60, Loss: 2.7364165782928467, Test Loss: 5.823096752166748\n",
      "Epoch: 70, Loss: 2.723911762237549, Test Loss: 5.793097019195557\n",
      "Epoch: 80, Loss: 2.71140718460083, Test Loss: 5.763095378875732\n",
      "Epoch: 90, Loss: 2.6989026069641113, Test Loss: 5.733094215393066\n",
      "Epoch: 100, Loss: 2.6863975524902344, Test Loss: 5.703093528747559\n",
      "Epoch: 110, Loss: 2.6738929748535156, Test Loss: 5.673093318939209\n",
      "Epoch: 120, Loss: 2.6613881587982178, Test Loss: 5.643091678619385\n",
      "Epoch: 130, Loss: 2.648883581161499, Test Loss: 5.613091468811035\n",
      "Epoch: 140, Loss: 2.636378526687622, Test Loss: 5.583090305328369\n",
      "Epoch: 150, Loss: 2.623873710632324, Test Loss: 5.553089618682861\n",
      "Epoch: 160, Loss: 2.6113688945770264, Test Loss: 5.523088455200195\n",
      "Epoch: 170, Loss: 2.5988640785217285, Test Loss: 5.493088245391846\n",
      "Epoch: 180, Loss: 2.5863592624664307, Test Loss: 5.46308708190918\n",
      "Epoch: 190, Loss: 2.573854446411133, Test Loss: 5.433086395263672\n",
      "Epoch: 200, Loss: 2.561349868774414, Test Loss: 5.403085231781006\n",
      "Epoch: 210, Loss: 2.548845052719116, Test Loss: 5.373084545135498\n",
      "Epoch: 220, Loss: 2.5363402366638184, Test Loss: 5.343083381652832\n",
      "Epoch: 230, Loss: 2.5238354206085205, Test Loss: 5.313083171844482\n",
      "Epoch: 240, Loss: 2.5113306045532227, Test Loss: 5.283082008361816\n",
      "Epoch: 250, Loss: 2.498825788497925, Test Loss: 5.25308084487915\n",
      "Epoch: 260, Loss: 2.486321210861206, Test Loss: 5.223081111907959\n",
      "Epoch: 270, Loss: 2.473816394805908, Test Loss: 5.193079948425293\n",
      "Epoch: 280, Loss: 2.4613115787506104, Test Loss: 5.163078308105469\n",
      "Epoch: 290, Loss: 2.4488067626953125, Test Loss: 5.133078098297119\n",
      "Epoch: 300, Loss: 2.4363017082214355, Test Loss: 5.103077411651611\n",
      "Epoch: 310, Loss: 2.423797130584717, Test Loss: 5.073076248168945\n",
      "Epoch: 320, Loss: 2.411292552947998, Test Loss: 5.043075084686279\n",
      "Epoch: 330, Loss: 2.3987877368927, Test Loss: 5.01307487487793\n",
      "Epoch: 340, Loss: 2.3862829208374023, Test Loss: 4.983073711395264\n",
      "Epoch: 350, Loss: 2.3737781047821045, Test Loss: 4.953073024749756\n",
      "Epoch: 360, Loss: 2.3612732887268066, Test Loss: 4.92307186126709\n",
      "Epoch: 370, Loss: 2.348768472671509, Test Loss: 4.893071174621582\n",
      "Epoch: 380, Loss: 2.33626389503479, Test Loss: 4.863070487976074\n",
      "Epoch: 390, Loss: 2.323758840560913, Test Loss: 4.833070278167725\n",
      "Epoch: 400, Loss: 2.3112542629241943, Test Loss: 4.8030686378479\n",
      "Epoch: 410, Loss: 2.2987494468688965, Test Loss: 4.773067951202393\n",
      "Epoch: 420, Loss: 2.2862446308135986, Test Loss: 4.743067264556885\n",
      "Epoch: 430, Loss: 2.2737395763397217, Test Loss: 4.713066101074219\n",
      "Epoch: 440, Loss: 2.261235237121582, Test Loss: 4.683064937591553\n",
      "Epoch: 450, Loss: 2.248730421066284, Test Loss: 4.653064250946045\n",
      "Epoch: 460, Loss: 2.2362253665924072, Test Loss: 4.623064041137695\n",
      "Epoch: 470, Loss: 2.2237205505371094, Test Loss: 4.5930633544921875\n",
      "Epoch: 480, Loss: 2.2112157344818115, Test Loss: 4.5630621910095215\n",
      "Epoch: 490, Loss: 2.1987109184265137, Test Loss: 4.5330610275268555\n",
      "Epoch: 500, Loss: 2.186206102371216, Test Loss: 4.503060817718506\n",
      "Epoch: 510, Loss: 2.173701524734497, Test Loss: 4.47305965423584\n",
      "Epoch: 520, Loss: 2.161196708679199, Test Loss: 4.443058490753174\n",
      "Epoch: 530, Loss: 2.1486918926239014, Test Loss: 4.413058280944824\n",
      "Epoch: 540, Loss: 2.1361870765686035, Test Loss: 4.383057117462158\n",
      "Epoch: 550, Loss: 2.1236824989318848, Test Loss: 4.35305643081665\n",
      "Epoch: 560, Loss: 2.111177682876587, Test Loss: 4.323055267333984\n",
      "Epoch: 570, Loss: 2.098672866821289, Test Loss: 4.293054580688477\n",
      "Epoch: 580, Loss: 2.086168050765991, Test Loss: 4.2630534172058105\n",
      "Epoch: 590, Loss: 2.0736632347106934, Test Loss: 4.233052730560303\n",
      "Epoch: 600, Loss: 2.0611584186553955, Test Loss: 4.203052043914795\n",
      "Epoch: 610, Loss: 2.0486536026000977, Test Loss: 4.173050880432129\n",
      "Epoch: 620, Loss: 2.036149024963379, Test Loss: 4.143050670623779\n",
      "Epoch: 630, Loss: 2.023643970489502, Test Loss: 4.113049030303955\n",
      "Epoch: 640, Loss: 2.011139154434204, Test Loss: 4.0830488204956055\n",
      "Epoch: 650, Loss: 1.9986345767974854, Test Loss: 4.053047180175781\n",
      "Epoch: 660, Loss: 1.986129641532898, Test Loss: 4.023046970367432\n",
      "Epoch: 670, Loss: 1.9736249446868896, Test Loss: 3.993046283721924\n",
      "Epoch: 680, Loss: 1.9611200094223022, Test Loss: 3.963045597076416\n",
      "Epoch: 690, Loss: 1.948615312576294, Test Loss: 3.933044910430908\n",
      "Epoch: 700, Loss: 1.9361107349395752, Test Loss: 3.903043746948242\n",
      "Epoch: 710, Loss: 1.9236056804656982, Test Loss: 3.8730430603027344\n",
      "Epoch: 720, Loss: 1.91110098361969, Test Loss: 3.8430421352386475\n",
      "Epoch: 730, Loss: 1.8985962867736816, Test Loss: 3.8130412101745605\n",
      "Epoch: 740, Loss: 1.8860913515090942, Test Loss: 3.7830400466918945\n",
      "Epoch: 750, Loss: 1.8735865354537964, Test Loss: 3.7530393600463867\n",
      "Epoch: 760, Loss: 1.8610820770263672, Test Loss: 3.723038911819458\n",
      "Epoch: 770, Loss: 1.8485771417617798, Test Loss: 3.693037986755371\n",
      "Epoch: 780, Loss: 1.8360722064971924, Test Loss: 3.663037061691284\n",
      "Epoch: 790, Loss: 1.8235673904418945, Test Loss: 3.6330361366271973\n",
      "Epoch: 800, Loss: 1.8110626935958862, Test Loss: 3.6030352115631104\n",
      "Epoch: 810, Loss: 1.7985577583312988, Test Loss: 3.5730345249176025\n",
      "Epoch: 820, Loss: 1.7860530614852905, Test Loss: 3.5430335998535156\n",
      "Epoch: 830, Loss: 1.7735482454299927, Test Loss: 3.5130326747894287\n",
      "Epoch: 840, Loss: 1.7610435485839844, Test Loss: 3.4830322265625\n",
      "Epoch: 850, Loss: 1.7485384941101074, Test Loss: 3.453030824661255\n",
      "Epoch: 860, Loss: 1.7360340356826782, Test Loss: 3.423030376434326\n",
      "Epoch: 870, Loss: 1.7235292196273804, Test Loss: 3.3930296897888184\n",
      "Epoch: 880, Loss: 1.711024284362793, Test Loss: 3.3630285263061523\n",
      "Epoch: 890, Loss: 1.6985195875167847, Test Loss: 3.3330278396606445\n",
      "Epoch: 900, Loss: 1.6860147714614868, Test Loss: 3.3030271530151367\n",
      "Epoch: 910, Loss: 1.673510193824768, Test Loss: 3.2730259895324707\n",
      "Epoch: 920, Loss: 1.6610052585601807, Test Loss: 3.243025541305542\n",
      "Epoch: 930, Loss: 1.6485004425048828, Test Loss: 3.213024854660034\n",
      "Epoch: 940, Loss: 1.6359955072402954, Test Loss: 3.183023691177368\n",
      "Epoch: 950, Loss: 1.623490810394287, Test Loss: 3.1530230045318604\n",
      "Epoch: 960, Loss: 1.6109861135482788, Test Loss: 3.1230220794677734\n",
      "Epoch: 970, Loss: 1.5984811782836914, Test Loss: 3.0930211544036865\n",
      "Epoch: 980, Loss: 1.585976481437683, Test Loss: 3.0630202293395996\n",
      "Epoch: 990, Loss: 1.5734716653823853, Test Loss: 3.0330193042755127\n",
      "Epoch: 1000, Loss: 1.5609668493270874, Test Loss: 3.003018617630005\n",
      "Epoch: 1010, Loss: 1.548462152481079, Test Loss: 2.973017930984497\n",
      "Epoch: 1020, Loss: 1.5359573364257812, Test Loss: 2.94301700592041\n",
      "Epoch: 1030, Loss: 1.5234525203704834, Test Loss: 2.9130165576934814\n",
      "Epoch: 1040, Loss: 1.510947823524475, Test Loss: 2.8830153942108154\n",
      "Epoch: 1050, Loss: 1.4984430074691772, Test Loss: 2.8530144691467285\n",
      "Epoch: 1060, Loss: 1.4859381914138794, Test Loss: 2.8230135440826416\n",
      "Epoch: 1070, Loss: 1.473433494567871, Test Loss: 2.7930126190185547\n",
      "Epoch: 1080, Loss: 1.4609286785125732, Test Loss: 2.763011932373047\n",
      "Epoch: 1090, Loss: 1.4484238624572754, Test Loss: 2.73301100730896\n",
      "Epoch: 1100, Loss: 1.4359190464019775, Test Loss: 2.703010082244873\n",
      "Epoch: 1110, Loss: 1.4234143495559692, Test Loss: 2.6730093955993652\n",
      "Epoch: 1120, Loss: 1.4109095335006714, Test Loss: 2.6430084705352783\n",
      "Epoch: 1130, Loss: 1.398404836654663, Test Loss: 2.6130077838897705\n",
      "Epoch: 1140, Loss: 1.3859000205993652, Test Loss: 2.5830068588256836\n",
      "Epoch: 1150, Loss: 1.3733952045440674, Test Loss: 2.553006172180176\n",
      "Epoch: 1160, Loss: 1.360890507698059, Test Loss: 2.523005247116089\n",
      "Epoch: 1170, Loss: 1.3483856916427612, Test Loss: 2.493004322052002\n",
      "Epoch: 1180, Loss: 1.3358807563781738, Test Loss: 2.463003635406494\n",
      "Epoch: 1190, Loss: 1.323376178741455, Test Loss: 2.4330029487609863\n",
      "Epoch: 1200, Loss: 1.3108713626861572, Test Loss: 2.4030020236968994\n",
      "Epoch: 1210, Loss: 1.2983665466308594, Test Loss: 2.3730010986328125\n",
      "Epoch: 1220, Loss: 1.2858614921569824, Test Loss: 2.3430001735687256\n",
      "Epoch: 1230, Loss: 1.2733567953109741, Test Loss: 2.3129994869232178\n",
      "Epoch: 1240, Loss: 1.2608522176742554, Test Loss: 2.282998561859131\n",
      "Epoch: 1250, Loss: 1.2483471632003784, Test Loss: 2.252998113632202\n",
      "Epoch: 1260, Loss: 1.2358424663543701, Test Loss: 2.2229971885681152\n",
      "Epoch: 1270, Loss: 1.2233376502990723, Test Loss: 2.192996025085449\n",
      "Epoch: 1280, Loss: 1.2108328342437744, Test Loss: 2.1629953384399414\n",
      "Epoch: 1290, Loss: 1.1983281373977661, Test Loss: 2.1329948902130127\n",
      "Epoch: 1300, Loss: 1.1858233213424683, Test Loss: 2.1029934883117676\n",
      "Epoch: 1310, Loss: 1.1733185052871704, Test Loss: 2.0729925632476807\n",
      "Epoch: 1320, Loss: 1.160813808441162, Test Loss: 2.042991876602173\n",
      "Epoch: 1330, Loss: 1.1483089923858643, Test Loss: 2.012990951538086\n",
      "Epoch: 1340, Loss: 1.1358041763305664, Test Loss: 1.9829899072647095\n",
      "Epoch: 1350, Loss: 1.123299479484558, Test Loss: 1.952989935874939\n",
      "Epoch: 1360, Loss: 1.1107947826385498, Test Loss: 1.9229885339736938\n",
      "Epoch: 1370, Loss: 1.098289966583252, Test Loss: 1.8929874897003174\n",
      "Epoch: 1380, Loss: 1.085785150527954, Test Loss: 1.8629870414733887\n",
      "Epoch: 1390, Loss: 1.0732803344726562, Test Loss: 1.832985758781433\n",
      "Epoch: 1400, Loss: 1.0607755184173584, Test Loss: 1.8029851913452148\n",
      "Epoch: 1410, Loss: 1.04827082157135, Test Loss: 1.7729846239089966\n",
      "Epoch: 1420, Loss: 1.0357657670974731, Test Loss: 1.7429834604263306\n",
      "Epoch: 1430, Loss: 1.0232611894607544, Test Loss: 1.7129827737808228\n",
      "Epoch: 1440, Loss: 1.010756492614746, Test Loss: 1.6829819679260254\n",
      "Epoch: 1450, Loss: 0.9982514381408691, Test Loss: 1.6529812812805176\n",
      "Epoch: 1460, Loss: 0.9857468605041504, Test Loss: 1.6229801177978516\n",
      "Epoch: 1470, Loss: 0.9732419848442078, Test Loss: 1.5929794311523438\n",
      "Epoch: 1480, Loss: 0.9607372283935547, Test Loss: 1.5629786252975464\n",
      "Epoch: 1490, Loss: 0.9482324719429016, Test Loss: 1.5329777002334595\n",
      "Epoch: 1500, Loss: 0.9357276558876038, Test Loss: 1.5029767751693726\n",
      "Epoch: 1510, Loss: 0.9232228994369507, Test Loss: 1.4729760885238647\n",
      "Epoch: 1520, Loss: 0.9107180237770081, Test Loss: 1.4429752826690674\n",
      "Epoch: 1530, Loss: 0.898213267326355, Test Loss: 1.4129743576049805\n",
      "Epoch: 1540, Loss: 0.8857085704803467, Test Loss: 1.382973551750183\n",
      "Epoch: 1550, Loss: 0.8732038140296936, Test Loss: 1.3529727458953857\n",
      "Epoch: 1560, Loss: 0.8606989979743958, Test Loss: 1.3229718208312988\n",
      "Epoch: 1570, Loss: 0.8481941223144531, Test Loss: 1.292971134185791\n",
      "Epoch: 1580, Loss: 0.8356893658638, Test Loss: 1.262970209121704\n",
      "Epoch: 1590, Loss: 0.8231845498085022, Test Loss: 1.2329691648483276\n",
      "Epoch: 1600, Loss: 0.8106797933578491, Test Loss: 1.2029684782028198\n",
      "Epoch: 1610, Loss: 0.798175036907196, Test Loss: 1.172967791557312\n",
      "Epoch: 1620, Loss: 0.7856702208518982, Test Loss: 1.142966866493225\n",
      "Epoch: 1630, Loss: 0.7731655240058899, Test Loss: 1.1129661798477173\n",
      "Epoch: 1640, Loss: 0.760660707950592, Test Loss: 1.0829652547836304\n",
      "Epoch: 1650, Loss: 0.7481558918952942, Test Loss: 1.052964448928833\n",
      "Epoch: 1660, Loss: 0.7356510162353516, Test Loss: 1.0229634046554565\n",
      "Epoch: 1670, Loss: 0.7231462597846985, Test Loss: 0.9929632544517517\n",
      "Epoch: 1680, Loss: 0.7106415629386902, Test Loss: 0.9629618525505066\n",
      "Epoch: 1690, Loss: 0.6981368064880371, Test Loss: 0.9329609274864197\n",
      "Epoch: 1700, Loss: 0.6856319904327393, Test Loss: 0.9029601812362671\n",
      "Epoch: 1710, Loss: 0.6731271743774414, Test Loss: 0.8729592561721802\n",
      "Epoch: 1720, Loss: 0.6606224179267883, Test Loss: 0.8429583311080933\n",
      "Epoch: 1730, Loss: 0.64811772108078, Test Loss: 0.812957763671875\n",
      "Epoch: 1740, Loss: 0.6356127262115479, Test Loss: 0.7829568386077881\n",
      "Epoch: 1750, Loss: 0.6231080293655396, Test Loss: 0.7529560923576355\n",
      "Epoch: 1760, Loss: 0.6106032133102417, Test Loss: 0.7229554057121277\n",
      "Epoch: 1770, Loss: 0.5980984568595886, Test Loss: 0.6929543018341064\n",
      "Epoch: 1780, Loss: 0.5855936408042908, Test Loss: 0.6629537343978882\n",
      "Epoch: 1790, Loss: 0.5730888843536377, Test Loss: 0.6329525113105774\n",
      "Epoch: 1800, Loss: 0.5605841279029846, Test Loss: 0.6029518246650696\n",
      "Epoch: 1810, Loss: 0.5480793118476868, Test Loss: 0.5729511380195618\n",
      "Epoch: 1820, Loss: 0.5355744957923889, Test Loss: 0.5429500341415405\n",
      "Epoch: 1830, Loss: 0.5230698585510254, Test Loss: 0.512949526309967\n",
      "Epoch: 1840, Loss: 0.5105650424957275, Test Loss: 0.4829483926296234\n",
      "Epoch: 1850, Loss: 0.4980602562427521, Test Loss: 0.452947735786438\n",
      "Epoch: 1860, Loss: 0.48555538058280945, Test Loss: 0.4229467213153839\n",
      "Epoch: 1870, Loss: 0.4730505645275116, Test Loss: 0.3929459750652313\n",
      "Epoch: 1880, Loss: 0.4605458974838257, Test Loss: 0.36294540762901306\n",
      "Epoch: 1890, Loss: 0.44804108142852783, Test Loss: 0.33294451236724854\n",
      "Epoch: 1900, Loss: 0.4355362355709076, Test Loss: 0.3029436469078064\n",
      "Epoch: 1910, Loss: 0.42303141951560974, Test Loss: 0.27294257283210754\n",
      "Epoch: 1920, Loss: 0.41052666306495667, Test Loss: 0.24294203519821167\n",
      "Epoch: 1930, Loss: 0.3980219066143036, Test Loss: 0.21294090151786804\n",
      "Epoch: 1940, Loss: 0.38551709055900574, Test Loss: 0.18294014036655426\n",
      "Epoch: 1950, Loss: 0.37301236391067505, Test Loss: 0.15293894708156586\n",
      "Epoch: 1960, Loss: 0.3605075180530548, Test Loss: 0.12293872237205505\n",
      "Epoch: 1970, Loss: 0.34800276160240173, Test Loss: 0.09293773770332336\n",
      "Epoch: 1980, Loss: 0.33549800515174866, Test Loss: 0.06519622355699539\n",
      "Epoch: 1990, Loss: 0.3229931890964508, Test Loss: 0.04995700716972351\n"
     ]
    }
   ],
   "source": [
    "# Train and Testing the model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "epochs = 2000\n",
    "\n",
    "# Track the loss values with each epoch\n",
    "epoch_count = []\n",
    "loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training phase\n",
    "    # Set the model to train mode\n",
    "    linearModel.train()\n",
    "\n",
    "    # Forward porpagation/Forward pass (compute the output)\n",
    "    yPred = linearModel(x_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = lossFunction(yPred, y_train)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters(Optimization of model parameters)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing phase\n",
    "    # Set the model to evaluation mode\n",
    "    linearModel.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Forward porpagation\n",
    "        yTestPred = linearModel(x_test)\n",
    "\n",
    "        # Calculate the test loss\n",
    "        test_loss = lossFunction(yTestPred, y_test)\n",
    "        # print(linearModel.state_dict())\n",
    "\n",
    "    # Track the loss values\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        loss_values.append(loss)\n",
    "        test_loss_values.append(test_loss)\n",
    "\n",
    "    # Print the loss values\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Creaate models directory\n",
    "MODEL_PATH = Path(\"Models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create the path to save the model\n",
    "MODEL_NAME = \"02_Linear Regression V2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict\n",
    "torch.save(obj=linearModel.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envResearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
